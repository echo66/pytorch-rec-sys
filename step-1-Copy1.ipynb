{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implementing a Recommender System in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this notebook, we will learn how to implement a recommender system that \"discovers\"/\"generates\" latent feature vectors, for users and items, given the user ratings. TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We will use the Movielens dataset. TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Due to resource restrictions, we will be ajusting the model parameters using a batch approach. TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In order to perform model comparison, we will use several approaches: \n",
    "1. Mean rating prediction error, TODO\n",
    "2. Mean Dislike/Like accuracy, through rating thresholding, TODO\n",
    "3. Mean Dislike/Like accuracy, per batch user, through rating thresholding, TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Model assumptions:\n",
    "1. The model isn't \"time invariant\": TODO\n",
    "2. User taste doesn't change too much in a short time span: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Hyper-parameters:\n",
    "* Batchsize: TODO\n",
    "* Epochs: TODO\n",
    "* Lambda: TODO\n",
    "* Number of Latent features: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "1. After receiving new data from the users, how should we select past data to adapt the model?\n",
    "2. TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$U \\in \\mathbb{R}^{(\\#U x F)}$\n",
    "\n",
    "$I \\in \\mathbb{R}^{(\\#I x F)}$\n",
    "\n",
    "$R \\in \\mathbb{R}^{(\\#U x \\#I)}$, a sparse real matrix\n",
    "\n",
    "$T \\in \\mathbb{R}^{(\\#U x \\#I)}$, a sparse real matrix\n",
    "\n",
    "$M \\in \\mathbb{R}^{(\\#U x \\#I)}$, a sparse binary matrix\n",
    "\n",
    "$\\hat{R} = (U_f \\cdot I_f) \\odot M$\n",
    "\n",
    "$E = \\frac{1}{\\#R}(\\hat{R} - R)^2 + \\lambda (\\left\\lVert U_f \\right\\rVert_{2}^{2} + \\left\\lVert I_f \\right\\rVert_{2}^{2})$\n",
    "\n",
    "$E = \\sqrt{\\frac{1}{\\#R}(\\hat{R} - R)^2} + \\lambda (\\left\\lVert U_f \\right\\rVert_{2}^{2} + \\left\\lVert I_f \\right\\rVert_{2}^{2})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from datetime import date\n",
    "import sys\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Loading the ratings dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The original ratings dataset is not sorted by timestamp. To make training and evaluation easier, we created a sorted dataset. TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ratings = pd.read_csv('ratings_sorted_by_timestamp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>122</td>\n",
       "      <td>2</td>\n",
       "      <td>945544824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>172</td>\n",
       "      <td>1</td>\n",
       "      <td>945544871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1221</td>\n",
       "      <td>5</td>\n",
       "      <td>945544788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1441</td>\n",
       "      <td>4</td>\n",
       "      <td>945544871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1609</td>\n",
       "      <td>3</td>\n",
       "      <td>945544824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  userId  movieId  rating  timestamp\n",
       "0           0       1      122       2  945544824\n",
       "1           1       1      172       1  945544871\n",
       "2           2       1     1221       5  945544788\n",
       "3           3       1     1441       4  945544871\n",
       "4           4       1     1609       3  945544824"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9428842</th>\n",
       "      <td>9428842</td>\n",
       "      <td>99708</td>\n",
       "      <td>1302</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1120227243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0  userId  movieId  rating   timestamp\n",
       "9428842     9428842   99708     1302     4.5  1120227243"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Counting how many ratings, users and items do we have in the ratings data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "numberOfItems = np.max(ratings.movieId.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "numberOfUsers = np.max(ratings.userId.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "numberOfRatings = ratings.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24404096, 165201, 259137)\n"
     ]
    }
   ],
   "source": [
    "print((numberOfRatings, numberOfItems, numberOfUsers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_ratings_and_mask_submatrices_from_batch(batch, userIds, movieIds): \n",
    "    R = np.zeros((userIds.shape[0], movieIds.shape[0]))\n",
    "    M = np.zeros((userIds.shape[0], movieIds.shape[0]))\n",
    "    \n",
    "    movieIdsIndexes = {}\n",
    "    userIdsIndexes = {}\n",
    "       \n",
    "    for i in range(movieIds.shape[0]):\n",
    "        movieIdsIndexes[movieIds[i]] = i\n",
    "        \n",
    "    for i in range(userIds.shape[0]):\n",
    "        userIdsIndexes[userIds[i]] = i\n",
    "        \n",
    "    for entry in batch.iterrows():\n",
    "        row = entry[1]\n",
    "        \n",
    "        uid = int(row.userId)\n",
    "        mid = int(row.movieId)\n",
    "        \n",
    "        uIdx = userIdsIndexes[uid]\n",
    "        mIdx = movieIdsIndexes[mid]\n",
    "        \n",
    "        R[uIdx,mIdx] = row.rating\n",
    "        M[uIdx,mIdx] = 1\n",
    "    \n",
    "    return R, M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Training (v3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def slice_and_dice_user_and_item_matrices(userIds, itemIds, UFeats, IFeats, numberOfLatentFeatures): \n",
    "     # Create temporary user and item matrices.\n",
    "    U = np.zeros((userIds.shape[0],  numberOfLatentFeatures+1))\n",
    "    I = np.ones((itemIds.shape[0], numberOfLatentFeatures))\n",
    "\n",
    "    # Slice the tensors and assign those slices to the temporary matrices.\n",
    "    U[:,range(numberOfLatentFeatures+1)] = UFeats[userIds]\n",
    "    I[:,range(numberOfLatentFeatures)]   = IFeats[itemIds]\n",
    "\n",
    "    U = torch.Tensor(U)\n",
    "    I = torch.Tensor(I)\n",
    "\n",
    "    # Create Variables\n",
    "    ufeats = torch.autograd.Variable(U, requires_grad=True)\n",
    "    ifeats = torch.autograd.Variable(I, requires_grad=True)\n",
    "    \n",
    "    return ufeats, ifeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def test_prediction(dataset, UFeats, IFeats): \n",
    "    testSample = dataset.sample()\n",
    "    uId = testSample.userId.values[0]\n",
    "    mId = testSample.movieId.values[0]\n",
    "    rat = testSample.rating.values[0]\n",
    "\n",
    "    ufeats = UFeats[uId,0:numberOfLatentFeatures]\n",
    "    ubias = UFeats[uId,numberOfLatentFeatures]\n",
    "    ifeats = IFeats[mId,0:numberOfLatentFeatures]\n",
    "\n",
    "    pred = np.dot(ufeats,ifeats) + ubias\n",
    "\n",
    "    return (((uId, mId, rat), pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def extract_batch(dataset, i, maxBatchSize): \n",
    "    batchStart = (i-1) * maxBatchSize\n",
    "    \n",
    "    batchEnd   = i * maxBatchSize\n",
    "    \n",
    "    batch = trainDataset[batchStart:batchEnd]\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def calculate_prediction_and_costs(ufeats, ifeats, mask, targets, nRats, lamb):\n",
    "    \n",
    "    pred = torch.mm(ufeats, ifeats.t()) * mask\n",
    "\n",
    "    mse  = torch.sum((pred - targets) ** 2) / nRats\n",
    "\n",
    "    rmse = torch.sqrt(mse)\n",
    "\n",
    "    regl2_u = (1/ufeats.size()[0]) * torch.sum(ufeats * ufeats)\n",
    "    #regl2_u = torch.sum(ufeats)\n",
    "\n",
    "    regl2_i = (1/ifeats.size()[0]) *torch.sum(ifeats * ifeats)\n",
    "    #regl2_i = torch.sum(ifeats)\n",
    "\n",
    "    # If prediction is bigger than the maximum allowed, \n",
    "    # put an additional penalization.\n",
    "    #aboveMaximumDifCost = torch.sum((pred > 5).float() - 5)\n",
    "    \n",
    "    #batchCost = rmse + (lamb * (regl2_u + regl2_i)) + aboveMaximumDifCost\n",
    "    batchCost = rmse + (lamb * (regl2_u + regl2_i))\n",
    "\n",
    "    return pred, mse, rmse, batchCost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_like_dislike_agreement(predictions, targets, mask, likeThreshold=3): \n",
    "    targetLikesIdx = (targets >= likeThreshold)\n",
    "    targetLikesCount = float(targetLikesIdx.sum())\n",
    "    likeAgreement = float((predictions[targetLikesIdx] >= likeThreshold).sum()) / targetLikesCount\n",
    "\n",
    "    targetDislikesIdx = (mask > 0) & (targets < likeThreshold)\n",
    "    targetDislikesCount = float(targetDislikesIdx.sum())\n",
    "    dislikeAgreement = float((predictions[targetDislikesIdx] < likeThreshold).sum()) / targetDislikesCount\n",
    "    \n",
    "    return likeAgreement, dislikeAgreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def single_batch_training(batch, UFeats, IFeats, lamb): \n",
    "    stats = {\n",
    "        'counts': {\n",
    "            'users': 0.0, 'items': 0.0, 'ratings': 0.0, 'sparsity': 0.0\n",
    "        }, \n",
    "        'time': {\n",
    "            'slicing_and_dicing_UFeats_IFeats': 0.0, \n",
    "            'building_ratings_and_mask_submatrices': 0.0, \n",
    "            'forward': 0.0, 'backward': 0.0, 'optimizer': 0.0, \n",
    "            'updating_UFeats_IFeats': 0.0\n",
    "        }, \n",
    "        'costs': {\n",
    "            'mse': 0.0, 'rmse': 0.0, 'total': 0.0\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Extract the list of unique user ids for the batch.\n",
    "    # Then, sort that list.\n",
    "    userIds = batch.userId.unique()\n",
    "    userIds.sort()\n",
    "\n",
    "    # Extract the list of unique movie ids for the batch.\n",
    "    # Then, sort that list.\n",
    "    itemIds = batch.movieId.unique()\n",
    "    itemIds.sort()\n",
    "    \n",
    "    nUIds = userIds.shape[0]\n",
    "    nIIds = itemIds.shape[0]\n",
    "    nRats = batch.shape[0]\n",
    "    sparsity = float(nRats) / float((nUIds * nIIds))\n",
    "    \n",
    "    stats['counts']['users'] = nUIds\n",
    "    stats['counts']['users'] = nIIds\n",
    "    stats['counts']['ratings'] = nRats\n",
    "    stats['counts']['sparsity'] = sparsity\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # --------------------------------------------------------------------------------\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    ufeats, ifeats = slice_and_dice_user_and_item_matrices(userIds, itemIds, UFeats, IFeats, numberOfLatentFeatures)\n",
    "    ones = torch.Tensor(np.ones((itemIds.shape[0], 1)))\n",
    "    ones = torch.autograd.Variable(ones, requires_grad=False)\n",
    "    iifeats = torch.cat((ifeats, ones), 1)\n",
    "\n",
    "    t1 = time.time()\n",
    "    \n",
    "    stats['time']['slicing_and_dicing_UFeats_IFeats'] = (t1-t0)\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # --------------------------------------------------------------------------------\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Build ratings and mask matrices.\n",
    "    targets, mask = build_ratings_and_mask_submatrices_from_batch(batch, userIds, itemIds)\n",
    "    targets = torch.autograd.Variable(torch.Tensor(targets), requires_grad=False)\n",
    "    mask    = torch.autograd.Variable(torch.Tensor(mask), requires_grad=False)\n",
    "\n",
    "    t1 = time.time()\n",
    "    \n",
    "    stats['time']['building_ratings_and_mask_submatrices'] = (t1-t0)\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # --------------------------------------------------------------------------------\n",
    "\n",
    "    optimizer1 = torch.optim.RMSprop([ufeats, ifeats])\n",
    "    optimizer1.zero_grad()\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # --------------------------------------------------------------------------------\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    #pred, mse, rmse, batchCost = calculate_prediction_and_costs(ufeats, ifeats, mask, targets, lamb, nRats\n",
    "    pred, mse, rmse, batchCost = calculate_prediction_and_costs(ufeats, iifeats, mask, targets, nRats, lamb)\n",
    "\n",
    "    t1 = time.time()\n",
    "    \n",
    "    stats['time']['forward'] = (t1-t0)\n",
    "    \n",
    "    stats['costs']['mse']   = mse.data.numpy()[0]\n",
    "    stats['costs']['rmse']  = rmse.data.numpy()[0]\n",
    "    stats['costs']['total'] = batchCost.data.numpy()[0]\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # --------------------------------------------------------------------------------\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    batchCost.backward()\n",
    "\n",
    "    t1 = time.time()\n",
    "    \n",
    "    stats['time']['backward'] = (t1-t0)\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # --------------------------------------------------------------------------------\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    optimizer1.step()\n",
    "\n",
    "    t1 = time.time()\n",
    "    \n",
    "    stats['time']['optimizer'] = (t1-t0)\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # --------------------------------------------------------------------------------\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    UFeats[userIds] = ufeats.data.numpy()[:,0:(numberOfLatentFeatures+1)]\n",
    "\n",
    "    IFeats[itemIds] = ifeats.data.numpy()[:,0:numberOfLatentFeatures]\n",
    "\n",
    "    t1 = time.time()\n",
    "    \n",
    "    stats['time']['updating_UFeats_IFeats'] = (t1-t0)\n",
    "    \n",
    "    return stats, targets.data.numpy(), mask.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models_config = [\n",
    "    { 'id': '0', 'numberOfLatentFeatures': 10, 'lamb': 0.5, \n",
    "      'epochs': 10, 'randomSeed': 1000, 'maxBatchSize': 20000, \n",
    "      'likeThreshold': 3.5, 'valPerc': 0.1}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainDataset = ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lamb': 0.5, 'numberOfLatentFeatures': 10, 'randomSeed': 1000, 'likeThreshold': 3.5, 'valPerc': 0.1, 'epochs': 10, 'id': '0', 'maxBatchSize': 20000}\n",
      "---------------------------------------\n",
      "\t  Batch 1/1220\n",
      "\t\t Training counts:\n",
      "\t\t\t (#Likes, #Dislikes) : (11463,6537)\n",
      "\t\t\t (#Users, #Items, #Ratings): (36,4521,18000)\n",
      "\t\t Validation counts:\n",
      "\t\t\t (#Likes, #Dislikes) : (1393,607)\n",
      "\t\t\t (#Users, #Items, #Ratings): (205,1170,2000)\n",
      "\t\t epoch 0 | training RMSE: 1.37457\n",
      "\t\t epoch 1 | training RMSE: 0.99120\n",
      "\t\t epoch 2 | training RMSE: 0.90068\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-317-c7eabb7d8e2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mtotalRMSE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mstats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msingle_batch_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainBatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUFeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIFeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0mtotalRMSE\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'costs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rmse'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t\\t epoch %d | training RMSE: %.5f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'costs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rmse'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-254-d4cc3c25115f>\u001b[0m in \u001b[0;36msingle_batch_training\u001b[0;34m(batch, UFeats, IFeats, lamb)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# Build ratings and mask matrices.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_ratings_and_mask_submatrices_from_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserIds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitemIds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mmask\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-786a1cdd05cc>\u001b[0m in \u001b[0;36mbuild_ratings_and_mask_submatrices_from_batch\u001b[0;34m(batch, userIds, movieIds)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0muserIdsIndexes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muserIds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/dist-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36miterrows\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/dist-packages/pandas/core/series.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 data = _sanitize_array(data, index, dtype, copy,\n\u001b[0;32m--> 225\u001b[0;31m                                        raise_cast_failure=True)\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/dist-packages/pandas/core/series.pyc\u001b[0m in \u001b[0;36m_sanitize_array\u001b[0;34m(data, index, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[1;32m   2889\u001b[0m     \u001b[0;31m# This is to prevent mixed-type Series getting all casted to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2890\u001b[0m     \u001b[0;31m# NumPy string type, e.g. NaN --> '-1#IND'.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2891\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubarr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2892\u001b[0m         \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for config in models_config: \n",
    "    \n",
    "    np.random.seed(config['randomSeed'])\n",
    "    \n",
    "    batchStats = []\n",
    "    \n",
    "    print(config)\n",
    "    \n",
    "    numberOfLatentFeatures = config['numberOfLatentFeatures']\n",
    "    maxBatchSize = config['maxBatchSize']\n",
    "    numberOfBatches = int(np.ceil(float(trainDataset.shape[0]) / float(maxBatchSize)))\n",
    "    \n",
    "    # [] Create model parameters\n",
    "    \n",
    "    # The first line won't be used\n",
    "    UFeats = np.random.uniform(low=0.0, high=1.0, size=(numberOfUsers+1, numberOfLatentFeatures+1))\n",
    "    UFeats[0,:] = np.inf\n",
    "    \n",
    "    # The first line won't be used\n",
    "    IFeats = np.random.uniform(low=0.0, high=1.0, size=(numberOfItems+1, numberOfLatentFeatures))\n",
    "    IFeats[0,:] = np.inf\n",
    "    \n",
    "    \n",
    "    # [] Training\n",
    "    lamb   = config['lamb']\n",
    "    epochs = config['epochs']\n",
    "    valPerc = config['valPerc']\n",
    "    \n",
    "    for b in range(1,numberOfBatches-1): \n",
    "        likeThreshold = config['likeThreshold']\n",
    "        \n",
    "        # Extract train, validation and test datasets.\n",
    "        trainBatch = extract_batch(trainDataset, b,   maxBatchSize)\n",
    "        testBatch  = extract_batch(trainDataset, b+1, maxBatchSize)\n",
    "        \n",
    "        trainSize = trainBatch.shape[0]\n",
    "        valSize   = int(np.ceil(trainSize * valPerc))\n",
    "        trainSize = int(trainSize - valSize)\n",
    "        \n",
    "        trainBatch = trainBatch.head(trainSize)\n",
    "        valBatch   = trainBatch.tail(valSize)\n",
    "        \n",
    "        print('---------------------------------------')\n",
    "        \n",
    "        print('\\t  Batch %d/%d' % (b,numberOfBatches-1))\n",
    "        \n",
    "        trainStats = {\n",
    "            '#Ratings': 0, \n",
    "            '#Likes': 0, '#Dislikes': 0, \n",
    "            '#Users': 0, '#Items': 0, \n",
    "            'avgRMSE': 0\n",
    "        }\n",
    "        \n",
    "        # Extract the user and item IDs.\n",
    "        trainUserIds = trainBatch.userId.unique()\n",
    "        trainUserIds.sort()\n",
    "        trainItemIds = trainBatch.movieId.unique()\n",
    "        trainItemIds.sort()\n",
    "        \n",
    "        valUserIds = valBatch.userId.unique()\n",
    "        valUserIds.sort()\n",
    "        valItemIds = valBatch.movieId.unique()\n",
    "        valItemIds.sort()\n",
    "        valTargets, valMask = build_ratings_and_mask_submatrices_from_batch(valBatch, valUserIds, valItemIds)\n",
    "        \n",
    "        testUserIds = testBatch.userId.unique()\n",
    "        testUserIds.sort()\n",
    "        testItemIds = testBatch.movieId.unique()\n",
    "        testItemIds.sort()\n",
    "        \n",
    "        # Count target likes & dislikes.\n",
    "        totalNumberOfLikesInTrainBatch = trainBatch[trainBatch.rating >= likeThreshold].shape[0]\n",
    "        totalNumberOfDislikesInTrainBatch = trainBatch[trainBatch.rating < likeThreshold].shape[0]\n",
    "        totalNumberOfLikesInValidationBatch = valBatch[valBatch.rating >= likeThreshold].shape[0]\n",
    "        totalNumberOfDislikesValidationBatch = valBatch[valBatch.rating < likeThreshold].shape[0]\n",
    "        totalNumberOfLikesInTestBatch = testBatch[testBatch.rating >= likeThreshold].shape[0]\n",
    "        totalNumberOfDislikesInTestBatch = testBatch[testBatch.rating < likeThreshold].shape[0]\n",
    "        \n",
    "        # Count the ratings.\n",
    "        totalRatingsInTrainBatch      = trainBatch.shape[0]\n",
    "        totalRatingsInValidationBatch = valBatch.shape[0]\n",
    "        totalRatingsInTestBatch       = testBatch.shape[0]\n",
    "        \n",
    "        # Print the counts.\n",
    "        print('\\t\\t Training counts:')\n",
    "        print('\\t\\t\\t (#Likes, #Dislikes) : (%d,%d)' % (totalNumberOfLikesInTrainBatch, totalNumberOfDislikesInTrainBatch))\n",
    "        print('\\t\\t\\t (#Users, #Items, #Ratings): (%d,%d,%d)' % (valUserIds.shape[0], trainItemIds.shape[0], totalRatingsInTrainBatch))\n",
    "        \n",
    "        print('\\t\\t Validation counts:')\n",
    "        print('\\t\\t\\t (#Likes, #Dislikes) : (%d,%d)' % (totalNumberOfLikesInValidationBatch, totalNumberOfDislikesValidationBatch))\n",
    "        print('\\t\\t\\t (#Users, #Items, #Ratings): (%d,%d,%d)' % (trainUserIds.shape[0], valItemIds.shape[0], totalRatingsInValidationBatch))\n",
    "        \n",
    "        # TODO: rearrange the code to include the validation set!\n",
    "        \n",
    "        totalRMSE = 0.0\n",
    "        for e in range(epochs):\n",
    "            stats, T, M = single_batch_training(trainBatch, UFeats, IFeats, lamb)\n",
    "            totalRMSE += stats['costs']['rmse']\n",
    "            print('\\t\\t epoch %d | training RMSE: %.5f' % (e, stats['costs']['rmse']))\n",
    "\n",
    "        ufeats, ifeats = slice_and_dice_user_and_item_matrices(userIds, itemIds, UFeats, IFeats, numberOfLatentFeatures)\n",
    "        ifeats = torch.cat((ifeats, torch.autograd.Variable(torch.Tensor(np.ones((itemIds.shape[0], 1))), requires_grad=False)), 1)\n",
    "\n",
    "        targets, mask = build_ratings_and_mask_submatrices_from_batch(trainBatch, userIds, itemIds)\n",
    "        targets = torch.autograd.Variable(torch.Tensor(targets), requires_grad=False)\n",
    "        mask    = torch.autograd.Variable(torch.Tensor(mask), requires_grad=False)\n",
    "\n",
    "        pred, _, rmse, _ = calculate_prediction_and_costs(ufeats, ifeats, mask, targets, nRats, 0)\n",
    "        pred = pred.data.numpy()\n",
    "        targets = targets.data.numpy()\n",
    "        mask = mask.data.numpy()\n",
    "        \n",
    "        likeAgreement, dislikeAgreement = calculate_like_dislike_agreement(pred, targets, mask, likeThreshold)\n",
    "        \n",
    "        trainStats = {\n",
    "            '#Ratings': stats['counts']['ratings'], \n",
    "            '#Likes': totalNumberOfLikesInTrainBatch, '#Dislikes': totalNumberOfDislikesInTrainBatch, \n",
    "            '#Users': stats['counts']['users'], '#Items': stats['counts']['items'], \n",
    "            'LikeAgreeRate': likeAgreement, 'DislikeAgreeRate': dislikeAgreement, \n",
    "            'avgRMSE': totalRMSE / epochs\n",
    "        }\n",
    "        \n",
    "        print('\\t\\t training avg RMSE: %.5f' % trainStats['avgRMSE'])\n",
    "        print('\\t\\t training Like Aggreement Rate: %.5f' % (trainStats['LikeAgreeRate']))\n",
    "        print('\\t\\t training Dislike Aggreement Rate: %.5f' % (trainStats['DislikeAgreeRate']))\n",
    "            \n",
    "        # [] Testing\n",
    "        nRats = testBatch.shape[0]\n",
    "\n",
    "        userIds = testBatch.userId.unique()\n",
    "        userIds.sort()\n",
    "        itemIds = testBatch.movieId.unique()\n",
    "        itemIds.sort()\n",
    "\n",
    "        ufeats, ifeats = slice_and_dice_user_and_item_matrices(userIds, itemIds, UFeats, IFeats, numberOfLatentFeatures)\n",
    "        ifeats = torch.cat((ifeats, torch.autograd.Variable(torch.Tensor(np.ones((itemIds.shape[0], 1))), requires_grad=False)), 1)\n",
    "\n",
    "        targets, mask = build_ratings_and_mask_submatrices_from_batch(testBatch, userIds, itemIds)\n",
    "        targets = torch.autograd.Variable(torch.Tensor(targets), requires_grad=False)\n",
    "        mask    = torch.autograd.Variable(torch.Tensor(mask), requires_grad=False)\n",
    "\n",
    "        pred, _, rmse, _ = calculate_prediction_and_costs(ufeats, ifeats, mask, targets, nRats, 0)\n",
    "        pred = pred.data.numpy()\n",
    "        targets = targets.data.numpy()\n",
    "        mask = mask.data.numpy()\n",
    "        \n",
    "        likeAgreement, dislikeAgreement = calculate_like_dislike_agreement(pred, targets, mask, likeThreshold)\n",
    "        \n",
    "        print('\\t\\t-------------------------------')\n",
    "        totalNumberOfLikesInTestBatch = testBatch[testBatch.rating >= likeThreshold].shape[0]\n",
    "        totalNumberOfDislikesInTestBatch = testBatch[testBatch.rating < likeThreshold].shape[0]\n",
    "        print('\\t\\t (#Likes, #Dislikes) : (%d,%d)' % (totalNumberOfLikesInTestBatch,totalNumberOfDislikesInTestBatch))\n",
    "        print('\\t\\t testing RMSE: %.5f' % (rmse.data.numpy()))\n",
    "        print('\\t\\t testing Like Aggreement Rate: %.5f' % (likeAgreement))\n",
    "        print('\\t\\t testing Dislike Aggreement Rate: %.5f' % (dislikeAgreement))\n",
    "        \n",
    "        testStats = {\n",
    "            '#Ratings': testBatch.shape[0], \n",
    "            '#Likes': totalNumberOfLikesInTestBatch, '#Dislikes': totalNumberOfDislikesInTestBatch, \n",
    "            '#Users': userIds.shape[0], '#Items': itemIds.shape[0], \n",
    "            'RMSE': rmse.data.numpy(), \n",
    "            'LikeAgreeRate': likeAgreement, 'DislikeAgreeRate': dislikeAgreement\n",
    "        }\n",
    "        \n",
    "        batchStats.append({\n",
    "            'trainStats': trainStats, 'testStats': testStats\n",
    "        })\n",
    "    \n",
    "        print('---------------------------------------')\n",
    "    \n",
    "    # [] Write model configuration, parameters and test results to files\n",
    "    if not os.path.exists('models'): \n",
    "        os.makedirs('models')\n",
    "        \n",
    "    modelDir = 'models/%s' % (config['id'])\n",
    "    if not os.path.exists(modelDir):\n",
    "        os.makedirs(modelDir)\n",
    "        \n",
    "    UFeatsFilepath = 'models/%s/UFeats' % (config['id'])\n",
    "    IFeatsFilepath = 'models/%s/IFeats' % (config['id'])\n",
    "    ConfigFilepath = 'models/%s/Config' % (config['id'])\n",
    "    StatsFilepath  = 'models/%s/Stats' % (config['id'])\n",
    "    \n",
    "    pickle.dump(UFeats, open(UFeatsFilepath, 'wb'))\n",
    "    pickle.dump(IFeats, open(IFeatsFilepath, 'wb'))\n",
    "    pickle.dump(config, open(ConfigFilepath, 'wb'))\n",
    "    pickle.dump(batchStats, open(StatsFilepath, 'wb'))\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_prediction(dataset=trainBatch, IFeats=IFeats, UFeats=UFeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
